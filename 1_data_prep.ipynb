{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c89d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbeb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da8e98",
   "metadata": {},
   "source": [
    "# 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a967ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    \"ALQ_L.XPT\": [\"SEQN\", \"ALQ121\"],\n",
    "    \"BMX_L.XPT\": [\"SEQN\", \"BMXWT\", \"BMXHT\", \"BMXBMI\"],\n",
    "    \"BPQ_L.XPT\": [\"SEQN\", \"BPQ020\"],\n",
    "    \"BPXO_L.XPT\": [\"SEQN\", \"BPXOSY1\",\"BPXODI1\",\"BPXOPLS1\"],\n",
    "    \"CBC_L.XPT\": [\"SEQN\",\"LBXHGB\"],\n",
    "    \"DEMO_L.XPT\": [\"SEQN\", \"RIAGENDR\",\"RIDAGEYR\",\"DMDBORN4\",\"DMDEDUC2\",\"DMDMARTZ\",\"RIDEXPRG\",\"DMDHHSIZ\"],\n",
    "    \"DIQ_L.XPT\": [\"SEQN\", \"DIQ010\"],\n",
    "    \"DPQ_L.XPT\": [\"SEQN\", \"DPQ010\",\"DPQ020\",\"DPQ030\",\"DPQ040\",\"DPQ050\",\"DPQ060\",\"DPQ070\",\"DPQ080\",\"DPQ090\"],\n",
    "    \"FNQ_L.XPT\": [\"SEQN\", \"FNQ410\",\"FNQ430\",\"FNQ440\",\"FNQ450\",\"FNQ460\",\"FNQ470\",\"FNQ490\"],\n",
    "    \"FOLATE_L.XPT\": [\"SEQN\", \"LBDRFOSI\"],\n",
    "    \"HDL_L.XPT\": [\"SEQN\", \"LBDHDDSI\"],\n",
    "    \"HIQ_L.XPT\": [\"SEQN\", \"HIQ011\"],\n",
    "    \"HOQ_L.XPT\": [\"SEQN\", \"HOD051\"],\n",
    "    \"HSCRP_L.XPT\": [\"SEQN\", \"LBXHSCRP\"],\n",
    "    \"HUQ_L.XPT\": [\"SEQN\", \"HUQ010\",\"HUQ030\",\"HUQ090\"],\n",
    "    \"INQ_L.XPT\": [\"SEQN\", \"INDFMMPI\",\"INQ300\"],\n",
    "    \"MCQ_L.XPT\": [\"SEQN\",\"MCQ160B\",\"MCQ160E\",\"MCQ160F\",\"MCQ160P\",\"MCQ220\"],\n",
    "    \"OCQ_L.XPT\": [\"SEQN\", \"OCD150\",\"OCQ180\",\"OCQ215\"],\n",
    "    \"OHQ_L.XPT\": [\"SEQN\", \"OHQ845\",\"OHQ680\"],\n",
    "    \"PAQ_L.XPT\": [\"SEQN\", \"PAD790Q\",\"PAD790U\",\"PAD680\"],\n",
    "    \"PBCD_L.XPT\": [\"SEQN\", \"LBXBPB\",\"LBXTHG\"],\n",
    "    \"SLQ_L.XPT\": [\"SEQN\", \"SLD012\",\"SLD013\"],\n",
    "    \"VID_L.XPT\": [\"SEQN\", \"LBXVD2MS\"],\n",
    "    \"WHQ_L.XPT\": [\"SEQN\", \"WHQ070\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c27598",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for filename, selected_columns in columns.items():\n",
    "    file_path = os.path.join(\"Raw_Datasets\", filename)\n",
    "    df, meta = pyreadstat.read_xport(file_path, encoding='cp1252')\n",
    "    df = df[selected_columns]\n",
    "    all_dfs.append(df)\n",
    "    \n",
    "merged_df = all_dfs[0]\n",
    "for df in all_dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=\"SEQN\", how=\"outer\")\n",
    "\n",
    "# Drop rows where all specified columns are empty\n",
    "columns_to_check = [\"DPQ010\", \"DPQ020\", \"DPQ030\", \"DPQ040\", \"DPQ050\", \"DPQ060\", \"DPQ070\", \"DPQ080\", \"DPQ090\"]\n",
    "merged_df = merged_df.dropna(subset=columns_to_check, how='all')\n",
    "\n",
    "merged_df.to_csv(\"merged_nhanes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b8477",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f134c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.read_csv(\"dictionary.csv\")\n",
    "lookup = dict(zip(dict_df['var_nhanes'], dict_df['var_id']))\n",
    "merged_df = merged_df.rename(columns=lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697f260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5519, 63)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4b0acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SEQN', 'habit01', 'physical01', 'physical02', 'physical03', 'com02',\n",
      "       'physical04', 'physical05', 'physical06', 'lab04', 'demo01', 'demo02',\n",
      "       'demo03', 'demo05', 'demo06', 'demo07', 'demo08', 'com03', 'target01',\n",
      "       'target02', 'target03', 'target04', 'target05', 'target06', 'target07',\n",
      "       'target08', 'target09', 'func01', 'func02', 'func03', 'func04',\n",
      "       'func05', 'func06', 'func07', 'lab06', 'lab08', 'healthcare01',\n",
      "       'demo10', 'lab10', 'com01', 'healthcare02', 'healthcare04', 'demo11',\n",
      "       'demo12', 'com07', 'com08', 'com09', 'com10', 'com13', 'job01', 'job02',\n",
      "       'job04', 'com15', 'com16', 'habit02', 'habit03', 'habit04', 'lab11',\n",
      "       'lab12', 'habit05', 'habit06', 'lab14', 'habit08'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12f4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns1 = [\"target01\", \"target02\", \"target03\", \"target04\", \"target05\", \"target06\", \"target07\", \"target08\", \"target09\"]\n",
    "for column_1 in columns1:\n",
    "    merged_df[column_1] = merged_df[column_1].replace(7, 0).replace(9, 0).fillna(0)\n",
    "\n",
    "columns2 = ['com02', 'com07', 'com08', 'com09', 'com10', 'com13', 'demo12', 'habit08',\n",
    "            'healthcare01', 'healthcare02', 'healthcare04', 'demo03']\n",
    "for column_2 in columns2:\n",
    "    merged_df[column_2] = merged_df[column_2].replace([2, 7, 9], 0).fillna(0)\n",
    "\n",
    "columns3 = ['com01', 'com15']\n",
    "for column_3 in columns3:\n",
    "    merged_df[column_3] = merged_df[column_3].replace([7, 9], 2).fillna(2)\n",
    "\n",
    "columns4 = ['com03']\n",
    "for column_4 in columns4:\n",
    "    merged_df[column_4] = merged_df[column_4].replace([2, 3, 7, 9], 0).fillna(0)\n",
    "\n",
    "columns5 = ['com16']\n",
    "for column_5 in columns5:\n",
    "    merged_df[column_5] = merged_df[column_5].replace([7, 9], 4).fillna(1)\n",
    "\n",
    "columns6 = ['demo05']\n",
    "for column_6 in columns6:\n",
    "    merged_df[column_6] = merged_df[column_6].replace([7, 9], 4).fillna(4)\n",
    "\n",
    "columns8 = ['func01', 'func02', 'func03', 'func04', 'func05', 'func06', 'func07']\n",
    "for column_8 in columns8:\n",
    "    merged_df[column_8] = merged_df[column_8].replace([7, 9], 1).fillna(1)\n",
    "\n",
    "columns10 = ['demo06']\n",
    "for column_10 in columns10:\n",
    "    merged_df[column_10] = merged_df[column_10].replace([77, 99], 3).fillna(3)\n",
    "\n",
    "columns11 = ['demo10']\n",
    "for column_11 in columns11:\n",
    "    merged_df[column_11] = merged_df[column_11].replace([77, 99], 5).fillna(5)\n",
    "\n",
    "columns12 = ['demo08']\n",
    "for column_12 in columns12:\n",
    "    merged_df[column_12] = merged_df[column_12].fillna(2)\n",
    "\n",
    "columns13 = ['demo07']\n",
    "for column_13 in columns13:\n",
    "    merged_df[column_13] = merged_df[column_13].replace([2, 3], 0).fillna(0)\n",
    "\n",
    "columns14 = ['habit01']\n",
    "for column_14 in columns14:\n",
    "    merged_df[column_14] = merged_df[column_14].replace({77: 5, 99: 5, 0: 11}).fillna(5)\n",
    "\n",
    "columns15 = ['habit02', 'habit04']\n",
    "for column_15 in columns15:\n",
    "    merged_df[column_15] = merged_df[column_15].replace([7777, 9999], 0).fillna(0)\n",
    "\n",
    "columns16 = ['job04']\n",
    "for column_16 in columns16:\n",
    "    merged_df[column_16] = merged_df[column_16].replace([77, 99], 0).fillna(0)\n",
    "\n",
    "columns17 = ['job01']\n",
    "for column_17 in columns17:\n",
    "    merged_df[column_17] = merged_df[column_17].replace({3: 0, 4: 0, 7: 0, 9: 0, 2: 1}).fillna(0)\n",
    "\n",
    "columns18 = ['job02']\n",
    "for column_18 in columns18:\n",
    "    merged_df[column_18] = merged_df[column_18].replace([77777, 99999], 0).fillna(0)\n",
    "\n",
    "merged_df['demo11'] = merged_df['demo11'].fillna(2.78)\n",
    "merged_df['habit05'] = merged_df['habit05'].fillna(7.7)\n",
    "merged_df['habit06'] = merged_df['habit06'].fillna(8.3)\n",
    "merged_df['lab06'] = merged_df['lab06'].fillna(1216)\n",
    "merged_df['lab08'] = merged_df['lab08'].fillna(1.41)\n",
    "merged_df['lab10'] = merged_df['lab10'].fillna(3.79)\n",
    "merged_df['lab11'] = merged_df['lab11'].fillna(0.956)\n",
    "merged_df['lab12'] = merged_df['lab12'].fillna(1.16)\n",
    "merged_df['lab14'] = merged_df['lab14'].fillna(4.8)\n",
    "\n",
    "merged_df['habit03'] = merged_df['habit03'].replace({\"D\": 365, \"M\": 12, \"W\": 52, \"Y\": 1})\n",
    "merged_df['habit03'] = merged_df['habit03'].where(merged_df.habit02 != 0, other=0)\n",
    "merged_df['habit09'] = merged_df.habit02 * merged_df.habit03\n",
    "merged_df = merged_df.drop(['habit02', 'habit03'], axis=1)\n",
    "\n",
    "merged_df.lab04 = np.where(merged_df.demo01 == 1, merged_df.lab04.fillna(14.8), merged_df.lab04.fillna(13.2))\n",
    "merged_df.physical01 = np.where(merged_df.demo01 == 1, merged_df.physical01.fillna(89.4), merged_df.physical01.fillna(77.9))\n",
    "merged_df.physical02 = np.where(merged_df.demo01 == 1, merged_df.physical02.fillna(174.8), merged_df.physical02.fillna(161.1))\n",
    "merged_df.physical03 = np.where(merged_df.demo01 == 1, merged_df.physical03.fillna(29.2), merged_df.physical03.fillna(30.0))\n",
    "merged_df.physical04 = np.where(merged_df.demo01 == 1, merged_df.physical04.fillna(125), merged_df.physical04.fillna(120))\n",
    "merged_df.physical05 = np.where(merged_df.demo01 == 1, merged_df.physical05.fillna(75), merged_df.physical05.fillna(74))\n",
    "merged_df.physical06 = np.where(merged_df.demo01 == 1, merged_df.physical06.fillna(69), merged_df.physical06.fillna(72))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e398b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "merged_df.to_csv(\"final_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06b72a",
   "metadata": {},
   "source": [
    "# 3. Training, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb73252",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73364c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"final_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967bc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['phq_sum'] = merged_df[[\"target01\",\"target02\",\"target03\",\"target04\",\"target05\",\n",
    "                                  \"target06\",\"target07\",\"target08\",\"target09\"]].sum(axis=1)\n",
    "merged_df = merged_df.drop(columns = [\"SEQN\",\"target01\",\"target02\",\"target03\",\"target04\",\"target05\",\n",
    "                                      \"target06\",\"target07\",\"target08\",\"target09\"]).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7181a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"final_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74795c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phq_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5514</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5516</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5519 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      phq_sum\n",
       "0         1.0\n",
       "1         2.0\n",
       "2         1.0\n",
       "3         0.0\n",
       "4         0.0\n",
       "...       ...\n",
       "5514      0.0\n",
       "5515      0.0\n",
       "5516      0.0\n",
       "5517      0.0\n",
       "5518      1.0\n",
       "\n",
       "[5519 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a09d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to 60-40 (training dataset and the rest)\n",
    "input_train, input_rest, output_train, output_rest = train_test_split (merged_df.iloc[:,:-1], \n",
    "                                                                       merged_df.iloc[:,-1:], \n",
    "                                                                       test_size=0.4, \n",
    "                                                                       random_state=SEED, \n",
    "                                                                       shuffle=True)\n",
    "    \n",
    "# Split to 20-20 (validation dataset and test dataset)\n",
    "input_validate, input_test, output_validate, output_test = train_test_split (input_rest, \n",
    "                                                                             output_rest, \n",
    "                                                                             test_size=0.5, \n",
    "                                                                             random_state=SEED, \n",
    "                                                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d9eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train.to_csv(\"Training_Validation_Test_Datasets/task2_input_train.csv\", index=False)\n",
    "input_validate.to_csv(\"Training_Validation_Test_Datasets/task2_input_validate.csv\", index=False)\n",
    "input_test.to_csv(\"Training_Validation_Test_Datasets/task2_input_test.csv\", index=False)\n",
    "\n",
    "output_train.to_csv(\"Training_Validation_Test_Datasets/task2_output_train.csv\", index=False)\n",
    "output_validate.to_csv(\"Training_Validation_Test_Datasets/task2_output_validate.csv\", index=False)\n",
    "output_test.to_csv(\"Training_Validation_Test_Datasets/task2_output_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f41c2",
   "metadata": {},
   "source": [
    "# 4. Data Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a069374",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"final_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b139e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'com02', 'com03', 'com07', 'com08', 'com09', 'com10',\n",
      "       'com13', 'com15', 'com16', 'demo01', 'demo02', 'demo03', 'demo05',\n",
      "       'demo06', 'demo07', 'demo08', 'demo10', 'demo11', 'demo12', 'func01',\n",
      "       'func02', 'func03', 'func04', 'func05', 'func06', 'func07', 'habit01',\n",
      "       'habit04', 'habit05', 'habit06', 'habit08', 'habit09', 'healthcare01',\n",
      "       'healthcare02', 'healthcare04', 'job01', 'job02', 'job04', 'lab04',\n",
      "       'lab06', 'lab08', 'lab10', 'lab11', 'lab12', 'lab14', 'physical01',\n",
      "       'physical02', 'physical03', 'physical04', 'physical05', 'physical06',\n",
      "       'phq_sum'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d185d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5519, 53)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b620cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = {\n",
    "        \"Male_Younger\": (merged_df['demo01'] == 1) & (merged_df['demo02'] >= 18) & (merged_df['demo02'] <= 55),\n",
    "        \"Male_Older\": (merged_df['demo01'] == 1) & (merged_df['demo02'] >= 56),\n",
    "        \"Female_Younger\": (merged_df['demo01'] == 2) & (merged_df['demo02'] >= 18) & (merged_df['demo02'] <= 55),\n",
    "        \"Female_Older\": (merged_df['demo01'] == 2) & (merged_df['demo02'] >= 56)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcebcb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male_Younger: 1227 entries\n",
      "Saved Male_Younger group data to Grouping_Datasets/Male_Younger.csv\n",
      "Male_Older: 1295 entries\n",
      "Saved Male_Older group data to Grouping_Datasets/Male_Older.csv\n",
      "Female_Younger: 1473 entries\n",
      "Saved Female_Younger group data to Grouping_Datasets/Female_Younger.csv\n",
      "Female_Older: 1524 entries\n",
      "Saved Female_Older group data to Grouping_Datasets/Female_Older.csv\n"
     ]
    }
   ],
   "source": [
    "for group_name, condition in conditions.items():\n",
    "    group_data = merged_df[condition]\n",
    "    group_size = len(group_data)\n",
    "    print(f\"{group_name}: {group_size} entries\")\n",
    "    \n",
    "    output_file = os.path.join(\"Grouping_Datasets\", f\"{group_name}.csv\")\n",
    "    group_data.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {group_name} group data to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546c1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
